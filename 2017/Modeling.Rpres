Modeling and Inference in R
========================================================
author: Dana Udwin (with props to Deirdre Fitzpatrick)
date: April 1, 2017
autosize: true

Overview
========================================================

Today we'll hit on:

* Linear regression (continuous outcome)

* Logistic regression (TRUE/FALSE outcome)

* Model evaluation

* Clustering

Introduction
========================================================

Modeling in R consists of:

* formula object
  + `Sepal.Length ~ Sepal.Width`

* model object
  + `lm()`

* observing your model
  + `anova()`, `plot()` or `summary()`

Putting it all together: 
```{r eval=FALSE}
# not going to execute
mdl <- lm(Sepal.Length ~ Sepal.Width, data=iris)
summary(mdl)
```

Simple Linear Regression (SLR)
========================================================

- Linear relationship between variables
- Response variable is continuous
- "Simple" = only one explanatory variable

Simple Linear Regression (SLR)
========================================================

##### Meet the data

<font size="5">
```{r fig.width=7, fig.height=7}
head(swiss)

# locate variables that appear linearly related
plot(swiss)
```
</font>

Simple Linear Regression (SLR)
========================================================

##### Fit the model

<font size="5">
```{r}
# response variable: Agriculture
# explanatory variable: Examination

mdl <- lm(Agriculture ~ Examination, data=swiss)
```
</font>

View and Analyze SLR Model
========================================================

<font size="5">
```{r fig.width=5, fig.height=5}
# examine coefficients
summary(mdl)

# analysis of variance
anova(mdl)

# series of plots
plot(mdl)
```
</font>

SLR Extras
========================================================

```{r}
# no intercept term
mdl <- lm(Agriculture ~ Examination - 1, data=swiss)
mdl <- lm(Agriculture ~ Examination + 0, data=swiss)

summary(mdl)
```

Type `?lm` in your console for arguments.

Wait...what even is an lm()?
========================================================

<center>
![](futurama.jpg)
</center>

Wait...what even is an lm()?
========================================================

An object of class "lm" contains:

* coefficients
* residuals
* fitted.values
* and other useful things (`?lm`)

An `lm()` model object also plays nice with other R utilities

<font size="4">
```{r}
# runif(5, 0, 100) samples 5 numbers from the uniform distribution (0, 100)
fake_data <- data.frame(Examination = runif(5, 0, 100))

predict(mdl, newdata=fake_data, interval="confidence")

r <- residuals(mdl)
head(r)

coef(mdl)

f <- fitted(mdl)
head(f)
```
</font>

Multiple Linear Regression
========================================================

- Response variable is still continuous
- Two or more independent explanatory variables

Let's model fertility using the Catholic and Education variables.

##### Check independence assumption

```{r fig.width=5, fig.height=5}
# install.packages('ggplot2')
library(ggplot2)
ggplot(swiss, aes(x = Catholic, y = Education)) + geom_point()
```

##### Cover ass better

```{r}
cor(swiss$Catholic, swiss$Education)
```

Multiple Linear Regression
========================================================

##### Fit model

```{r}
mdl <- lm(Fertility ~ Catholic + Education, data=swiss)
summary(mdl)
```

Partial F-Test
========================================================

Let's suppose a new model!

Fertility = $\beta_0$ + $\beta_1$ Education + $\beta_2$ Catholic + $\beta_3$ Infant.Mortality + $\epsilon$

Test hypothesis that $\beta_2 = \beta_3 = 0$.

<font size="5">
```{r}
reduced_mdl <- lm(Fertility ~ Education, data=swiss)
full_mdl <- lm(Fertility ~ Education + Catholic + Infant.Mortality, data=swiss)

anova(reduced_mdl, full_mdl) # you can pass in two models!
```
</font>

Or even more magically:
<font size="5">
```{r}
anova_magic <- anova(reduced_mdl, full_mdl)
class(anova_magic)
names(anova_magic)
anova_magic[["Pr(>F)"]]
```
</font>

Multiple Linear Regression...with interaction!
========================================================

Back to the model we fit earlier: Fertility = $\beta_0$ + $\beta_1$ Catholic + $\beta_2$ Education + $\epsilon$

As Education changes, does the effect of Catholic on Fertility change?

As Catholic changes, does the effect of Education on Fertility change?

<font size="5">
```{r}
mdl <- lm(Fertility ~ Catholic + Education + Catholic*Education, data=swiss) # yay
mdl <- lm(Fertility ~ Catholic + Education + Catholic:Education, data=swiss) # yay
mdl <- lm(Fertility ~ Catholic*Education, data=swiss) # yay
# mdl <- lm(Fertility ~ Catholic:Education, data=swiss) <-- NO, includes no additive terms

summary(mdl)
```
</font>

Multiple Linear Regression...with interaction!
========================================================

Recall our plot from earlier, to assess independence between Catholic and Education:

```{r fig.width=5, fig.height=5}
ggplot(swiss, aes(x = Catholic, y = Education)) + geom_point()
```

Looks like Catholic is either very small or very big.

```{r fig.width=5, fig.height=5}
summary(swiss$Catholic)
```

Multiple Linear Regression...with interaction!
========================================================

<font size="5">
```{r}
# more nifty notation
catholic_summary <- summary(swiss$Catholic)
class(catholic_summary)
catholic_summary["Median"]
catholic_summary[["Median"]]
```
</font>

Let's recode Catholic as a factor and see what happens.

<font size="5">
```{r}
Catholic_median <- summary(swiss$Catholic)[["Median"]]

# install.packages('dplyr')
# install.packages('mosaic')
library(dplyr)
library(mosaic)

swiss_new <- swiss %>% mutate(
  Catholic_factor = derivedFactor(low = Catholic <= Catholic_median, 
                           high = Catholic > Catholic_median))

head(swiss_new[c('Catholic', 'Catholic_factor')])
```
</font>

Multiple Linear Regression...with interaction!
========================================================

Test hunch.

```{r}
ggplot(swiss_new, aes(x = Education, y = Fertility)) + geom_point(aes(colour = Catholic_factor))
```

No visual evidence of interaction...but let's try anyways.

Multiple Linear Regression...with interaction!
========================================================

```{r}
mdl <- lm(Fertility ~ Catholic_factor*Education, data=swiss_new)
summary(mdl)
```

Logistic Regression
========================================================

* Response variable is categorical
* But we are still considering a linear relationship!

You can't have a linear relationship between, say, age and gender.

But you can have a linear relationship between age and the log odds that someone is female.

Enter logit function:

$$
\begin{equation}
\log \frac{p}{1-p} = \beta_0 + \beta_1 X_1 + \dots + \beta_n X_n
\end{equation}
$$

$p$ = probability of "success"

$\log$$\frac{p}{1-p}$ = "log odds"

$\beta_0$ = model intercept

$\beta_i$ = the coefficient on explanatory variable $X_i$

The Data
========================================================

```{r}
df <- as.data.frame(UCBAdmissions)
head(df)
```

(Enter `?UCBAdmissions` into console for deets.)

$p$ = probability of being admitted

$X_1$ = Gender

$X_2$ = Dept

A Little Finagling
========================================================

```{r}
# install.packages('reshape')
require(reshape)

# unravel
df <- untable(df[, c('Admit', 'Gender', 'Dept')], num=df[, 'Freq'])
rownames(df) <- seq(length=nrow(df))

# re-order levels of Admit
df <- mutate(df, Admit = relevel(Admit, 'Rejected'))

dim(df)
head(df)
```

Training
========================================================

Subset the data into training set (fit model) and testing set (test model).

```{r}
df_idx <- 1:nrow(df)
train_size <- floor(.8 * nrow(df))

train_idx <- sample(df_idx, size=train_size)
test_idx <- df_idx[!(df_idx %in% train_idx)]

train_df <- df[train_idx, ]
test_df <- df[test_idx, ]
```

Fit the Model
========================================================

`glm()` function (generalized linear model)

<font size="5">
```{r}
mdl <- glm(Admit ~ Gender, family=binomial(link='logit'), data=train_df)

summary(mdl)
```
</font>

Interpret Coefficients
========================================================

Recall: The model fits $\log \frac{p}{1-p} = \beta_0 + \beta_1 X_1$.

$\beta_0$ = `r coef(mdl)[1]`

$\beta_1$ = `r coef(mdl)[2]`

```{r}
coef(mdl)
```

When applicant is male, $X_1$ = 0 and the log odds of admission are equal to the intercept.

We expect the log odds of admission to increase by `r coef(mdl)[2]` when applicant is female.

Multiple Logistic Regression
========================================================

<font size="5">
```{r}
# pro tip: a period includes all variables as predictors
mdl <- glm(Admit ~ ., family=binomial(link='logit'), data=train_df)

summary(mdl)
```
</font>

Simpson's Paradox!

Test Model
========================================================

##### Receiver Operating Characteristic Curve

<font size="5">
```{r fig.height=5, fig.width=5}
#install.packages('ROCR')
library(ROCR)

prob_admit <- predict(mdl, newdata=test_df, type="response")
truth_admit <- test_df$Admit
pred <- prediction(prob_admit, truth_admit, label.ordering=c('Rejected', 'Admitted'))
perf <- performance(pred, "tpr", "fpr")
plot(perf)
```
</font>

##### Area Under the Curve

<font size="5">
```{r}
auc.tmp <- performance(pred, "auc")
auc <- as.numeric(auc.tmp@y.values)
auc
```
</font>

Test Model
========================================================

##### Confusion Matrix

<font size="6">
```{r}
# find threshold
roc_df <- data.frame(cut=perf@alpha.values[[1]], fpr=perf@x.values[[1]], tpr=perf@y.values[[1]])
head(roc_df)

dist_from_0.1 <- sapply(roc_df['fpr'], function(fpr) return(abs(fpr - 0.1)))
cut_idx <- which(dist_from_0.1 == min(dist_from_0.1))
cutoff <- roc_df[cut_idx, 'cut']
cutoff

# recode probabilities to Rejected if less than cut-off, else Admitted
pred_admit <- ifelse(prob_admit > cutoff, 'Admitted', 'Rejected')
```
</font>

Test Model
========================================================

##### Confusion Matrix

<font size="6">
```{r message=FALSE, warning=FALSE}
# install.packages('caret')
library(caret)

confusionMatrix(pred_admit, truth_admit, positive="Admitted")
```
</font>

Clustering
========================================================

This is a classification task that seeks to group items in a way that:

1. Minimizes variance within groups

2. Maximizes distance between groups

Here, we will use the `kmeans()` function to implement k-means clustering. This is an unsupervised (a.k.a. the model doesn't learn from ground truth labels) clustering algorithm that identifies $k$ group "centers" and assigns each data point to the group with the nearest centroid.

Data
========================================================

```{r}
# I'm sorry
head(iris)
```

Do a little scrubbing.
```{r}
df <- iris[, c('Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width')]
df <- na.omit(df)
df <- scale(df)
```

Determine Number of Clusters
========================================================

Elbow Method: Identify the number of clusters at which the sums of squared errors within groups drops.

<center>
```{r}
set.seed(1)
col_variance <- apply(df, 2, var)

# sum squared error for only 1 group
weighted_ss <- (nrow(df)-1)*sum(col_variance)

for (i in 2:15) {
  within_cluster_ss <- kmeans(df, centers=i)$withinss
  weighted_ss[i] <- sum(within_cluster_ss)
}

plot(1:15, weighted_ss, pch=20, type="b", xlab="Number of Clusters", ylab="Within Group SSE")
```
</center>

Implement K-Means
========================================================

```{r}
fit <- kmeans(df, centers=3)

aggregate(df, by=list(fit$cluster), FUN=mean)
```

How Did We Do?
========================================================

We implemented unsupervised clustering, but...the iris data set has a species column. Let's consider that ground truth for our clusters and compare with the k-means results.

```{r, warnings=FALSE}
df <- data.frame(df, Fit=fit$cluster)
df$Fit <- as.factor(df$Fit)
df["Species"] <- iris$Species

tbl <- table(df$Fit, df$Species)
print(tbl)

#install.packages('dplyr')
#install.packages('plyr')
require(dplyr)
require(plyr)
df <- df %>% mutate(Fit = revalue(Fit, c('1' = 'setosa', '2' = 'virginica', '3' = 'versicolor')))
```

How Did We Do?
========================================================

<font size="6">
```{r}
#install.packages('caret')
#install.packages('e1071')
require(caret)
require(e1071)

confusionMatrix(df$Fit, df$Species)
```
</font>

Thanks!
========================================================

dudwin@massmutual.com

(chat with me about MassMutual Data Labs + our data science development program located in Amherst)